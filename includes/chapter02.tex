%Empieza configuracion de capitulo
\setstretch{1.0}
\titleformat{\chapter}[block]{\Large\bfseries}{CHAPTER \Huge\thechapter\vspace{25 pt}}{0 pt}{\\\fontsize{26}{36}\selectfont}
\titlespacing{\chapter}{0 pt}{30 pt}{50 pt}[0 pt]
\titleformat{\section}{\Large\bfseries}{\thesection}{0 pt}{\hspace{30 pt}}
\titleformat{\subsection}{\large\bfseries}{\thesubsection}{0 pt}{\hspace{30 pt}}
\pagestyle{fancy}
\fancyhead[LO,LE]{\footnotesize\textit{\leftmark}}
\fancyhead[RO,RE]{\thepage}
\fancyfoot[CO,CE]{}
%Termina configuracion de capitulo

\chapter{State of the Art} %Cambia al nombre de tu capitulo
\setstretch{1.5} %Regresa el interlineado a 1.5

\normalsize

\noindent

This chapter describes recent research work on parallel and distributed 
computing in embedded platforms and clusters of embedded systems.

\section{Embedded Distributed Computing}
%introduction-> MPI

With the proliferation of high performance workstations and the emergence of
high speed networks in the early 1990s, the rise of interest in parallel and
distributed computing grow rapidly. The first clusters of computers required
communication protocols to share data and workload. In \cite{Salim} one of the
most popular paradigms for parallel and distributed computing is described:
Message Passing Interface, MPI. The main objective of MPI was to provide a
library for the development of parallel and distributed applications with
message passing primitives, as well as to deploy a host network to share the
workload among the workstations.

%MPICH

In \cite{Salim}, it is published MPICH,  the first practical implementation of
the MPI protocol. MPICH is a high performance and widely portable
implementation of the MPI.\cite{Gropp}. MPI was designed for high performance
computing on both, massively parallel machines and on workstation
clusters.\footnote{More details about MPICH are covered in Chapter 3}. It
provides libraries for C and Fortran programing languages. It allows the
development of applications to share data and workload between multiple
computing nodes. From the point of view of the programmer it is necessary to
specify what section of the code needs to be run in parallel at the nodes.
Although MPICH was one of the first MPI implementations, it is not the only
one; nowadays, there are multiple that follow the same principles.

%First MPICH for Embedded

MPI implementations are designed for clusters of high performance computers.
Clusters where it is expected to have a large amount of resources with
operating systems that enable distributed computing.
Despite of the fact that many researchers use MPI implementations in the high
performance computing community, there is a small group of people interested in
the same approach of parallel and distributed computing applied to embedded
systems. One of the first approaches was eMPICH \cite{McMahon}. eMPICH has two
design paradigms for embedded MPI: top-down and bottom-up.

\begin{itemize} 
\item The top-down approach focused on removing and simplifying
features of full MPI implementations, and reorganizing the code in order to
decrease the amount of unneeded code linked into an application. Some examples
of changes include: to remove support for data heterogeneity, removing error
checking and reducing multiple send modes. These changes reduce the complexity
of MPI to send, receive and handle messages.

\item The bottom-up approach implemented a full rewrite of the MPI protocol
First it was a very small, six-function version of eMPICH. Latter it was
created a sequence of eMPICH libraries by adding various functionality to the
base implementation.  
\end{itemize}

In \cite{McMahon} it is discussed that besides reducing the executable code
size of the MPI layer, there are other issues that need to be addressed within
the scope of embedded MPI. Many of them are related to the lack of proper
operating system for embedded platforms. The operating systems is responsible
to manage techniques for inter-processor communication. In order to create a
parallel and distributed computing system for embedded platforms it is
necessary to solve these problems first.

% Thread base solutions

\subsection{Thread-based Approach}

To achieve the implementation of MPI for embedded platforms it is required to
have several system resources for example: volatile and non-volatile memory, as
well as the use of a microprocessor instead of a low-voltage microcontroler
with comprehensive operating system support. Those resources were not available
in an embedded platform at the end of 1990s.  One of the projects that follows
this approach is AzequiaMPI \cite{Gallego}.

AzequiaMPI is an MPI implementation that uses threads instead of processes,
making MPI applications lighter. The AzequiaMPI protocol runs on the Texas
Instruments\textregistered\ TMS320C6000\textregistered\ family of digital
signal processors (DSPs). \footnote{The DSP is a specialized microprocessor
with its architecture optimized for digital signal processing tasks. DSPs are
broadly used in high performance real time signal processing applications.}.
Threads are used for small tasks, whereas processes are used for larger tasks.
Another difference between a thread and a process is that threads that run
within the same process share the same address space, whereas different
processes do not.

According to \cite{Gallego}, running an MPI node as a process imposes some
disadvantages, process context switches and synchronization are expensive.
Second, message passing between two MPI nodes in the same machine must go
through a system buffer, affecting  the communication efficiency. 

The idea that \cite{Gallego} proposed was radical, using threads instead of
processes. In the original idea presented by \cite{Salim}, the MPI node  is a
process \footnote{Other MPI implementations that do the same include MPICH and
OpenMPI}, in contrast in \cite{Gallego} the MPI node is a thread in a
thread-based implementation.

Before the publication of \cite{Gallego} the idea of thread was popular.  In
\cite{Tang} it is proposed TMPI (threaded MPI), a technique that  allows MPI
nodes to be executed safety and efficiently as threads. According to
\cite{Tang}, current MPI implementations on shared-memory machines map each MPI
node to an OS process, which can suffer serious performance degradation in the
presence of multiprogramming. To reduce the problem they propose a runtime
support that includes an efficient communication protocol that uses lock-free
data structures and takes advantage of address space sharing among threads. The
experiments on \cite{Tang} show that TMPI has significant performance
advantages in comparison to a traditional MPI approach.

Another example of thread-based techniques is \cite{Demaine}. In such work
TOMPI (thread only MPI) is presented. TOMPI is a threads only implementation of
MPI. While the implementation is partial it supports the commonly used MPI
features. The communication is designed to be efficient by using asynchronous
communication by default. The idea was to extend TOMPI to efficiently support
the entire MPI standard; however it was just an early proof of concept
prototype that implements just a reduced set of MPI primitives. 

Some tests (Figure 3 in \cite{Gallego}) compare the performance of AzequiaMPI,
TOMPI and MPICH in a single Linux PC. The results show that the thread-based
implementations double the performance of MPICH, on other hand, the performance
of AzequiaMPI is quite similar to the TOMPI project. 

The results in \cite{Gallego} conclude that better threads support would
contribute to simplify the AzequiaMPI solution, eliminating the need of an
operating system, improving the performance and reducing its memory footprint. 

%Process base approach
\subsection{Process-based Approach}

The work discussed in \cite{Saldana-Chow} and \cite{Williams} demonstrates that
MPI is the correct programming model in order to build a hybrid cluster of
embedded platforms hiding hardware complexities from the programmer and
promoting code portability. In \cite{Saldana-Chow} the authors describe a
lightweight subset MPI standard implementation called TMD-MPI to execute
parallel C programs across multiple FPGAs. A simple NoC \footnote{Network on a
chip (NoC) is a communication subsystem on an integrate in a system in a chip}
was developed to enable communications within and across FPGAs, on top of which
TMD-MPI can send and receive messages. 

The experiments in \cite{Saldana-Chow}  show that, for short messages,
communications between multiple processors can perform better than the network
of Pentium 3 machines using a 100Mb/s Ethernet network. TMD-MPI, does not
depend on the existence of an operating system for the functions implemented.
TMD-MPI is small enough that can work with internal RAM in an FPGA. Currently,
the size of the library is 8.7 KB, which makes it suitable for embedded
systems.

One of the most advanced process MPI implementations for embedded systems is
the Lightweight Message Passing Interface (LMPI)\cite{Abgaria}. The idea of
LMPI is separation of its server part (LMPI server) and the very thin client
part (LMPI client). Both parts can reside on different hardware, or on the same
hardware. Multiple clients can be associated with a server. LMPI servers
support full capability of MPI and can be implemented using pre-existing MPI
implementations. 

Although LMPI is dedicated to embedded systems, to demonstrate the benefits of
LMPI and to show some initial results, in \cite{Abgaria} it was  built an LMPI
server using MPICH on a non-embedded system. LMPI client consumes far less
computation and communication bandwidth than typical implementations of MPI,
like MPICH.  As a result, LMPI client is suitable for embedded systems with
limited computation power and memory. That work demonstrated the low overhead
of LMPI clients on Linux workstations, which is as low as 10\% of MPICH for two
benchmark applications.  LMPI clients are highly portable because they don't
rely on the operating system support. All they require from the embedded system
is networking support to the LMPI server.

These light and limited versions of MPI either process-based or thread-based
are relevant proofs of the raising role of MPI in the embedded world. However,
they have challenges, one of them is portability. The portability of code among
different platforms needs to be considered. For example MPI programs that run
in a traditional Linux cluster (eg. a Bewolf Cluster) should also be able to
run in a cluster of embedded platforms  by code recompilation.  In
\cite{Gallego} it is shown that the support of an operating system is essential
for portability to success.

\section{Multicore Communication Application Program Interface (MCAPI)}

The increment in the complexity of the new embedded applications started to
require more specialized integrated circuits that could help the microprocessor
with all these task in parallel. As we know modern embedded platforms use
multi-core microprocessors to solve this problem.

The lack of a framework to share the workload among the multi core of these new embedded
applications was an initial problem. One of the first Multicore Communication systems is MCPI.
According to \cite{MCAPI} the purpose of MCAPI, which is a message-passing API,
is \textit{To capture the basic elements of communication and synchronization
that are required for closely distributed (multiple cores on a chip and/or
chips on a board) embedded systems} 

There are already several programming models and tools for multiprocessor
System-on-Chip (MPSoC) programming \cite{Wolf} \cite{Matilainen}. These tools
help in partitioning and mapping the problem to the specific platform.
However, these solutions are too heavy-weight and doesn't support a wide range
of embedded platforms. 

On the other side MCAPI provides a limited number of calls with sufficient
communication functionality. MCAPI is scalable and can support virtually any
number of cores, each with a different processing architecture and each running
the same or a different operating system.

Some inter-processor communications protocols \cite{Wolf} require a full TCP/IP
stack to exchange data, creating a bloated memory footprint. MCAPI, on the
other hand, does not require this and is much more lightweight. At the end, an
application using MCAPI will see an standard set of function calls to send and
receive data to and from any core in the system. 

MCAPI is not a protocol specification. This
is an implementation issue, due to the fact that with multiple vendors of
embedded platforms a standard is becoming a necessity. Another disadvantage is
that MCAPI just send an receive data within the cores of the embedded platform
not among multiple embedded platforms. 

As we have seen for that purpose exist MPI (either process or thread base ).
There are key differences between the two technologies. MPI can be used to
create programs that adapt to the resources available in a dynamic network of
devices (embedded platforms, servers, computers). If one system goes down the
workload is re distributed the next time. Besides, MPI focuses on the share of
workload (wither process base or thread base ) and exchanging messages between
them. It doesn't matter if the devices are side-by-side or half a world apart. 

MPI is not a good fit for communication within the cores of a
server, where different cores might run and avoid bottlenecks in bus and memory
access. This is where MCAPI comes in, Allowing a light communication among the
cores.

\section{Clusters of Embedded Systems}

Despite of the type of MPI implementation either process-based or,
thread-based, it is proved that when multiple embedded processors are
available, the workload can be distributed in such a way that each processor
runs at a very low-power level; meanwhile, the performance is compensated by
parallelism.  Based on this idea some efforts have been done in \cite{Liu} and
\cite{Weglarz}. Those propose the idea of partitioning the computation onto a
multi-processor architecture that consumes significantly less power than a
single processor.

In \cite{Weglarz} the authors examine the use of multiple constrained
processors running at lowered voltage and frequency to perform a similar amount
of work in a shorter time and lower power than a uniprocessor.  The workload
studied in that work is a video encoder. As a result it is proved that four
processors sharing the workload at a lower frequency can save up to as 56\% of
energy compared to a uniprocessor running the workload. According to
\cite{Weglarz} a future work will involve developing an analytical model that
estimates the minimum amount of parallelism that is needed for a multiprocessor
to save energy.

Another example of a distributed embedded application is presented in
\cite{Liu}. In that work the author implemented an image processing algorithm
with MPI on a low voltage computer. As a result the author shows that when
multiple low-voltage processors are available, the workload can be distributed
in such a way that each processor runs at a very low-power level; meanwhile the
performance is compensated by parallelism. The author also confirms that
distributed programming tools are available to leverage the design and
exploration of distributed embedded applications. However he emphasizes that
the communication mechanism of MPI needs further study.

As can be seen there is a need to make new experiments with the latest
technology available. Nowadays, we have ultra-low-voltage microprocessor
platforms, customized operating systems for these platforms and  innovative
technologies for achieving MPI implementations. Few works, like those described
in \cite{Victor-Marcos} and \cite{Victor-Marcos-elc} have address the problem
of parallel and distributing computing with embedded platforms. These design
parameters are considered for the development of this work.

\newpage

\clearpage
