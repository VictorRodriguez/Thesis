%Empieza configuracion de capitulo
\setstretch{1.0}
\titleformat{\chapter}[block]{\Large\bfseries}{CHAPTER \Huge\thechapter\vspace{25 pt}}{0 pt}{\\\fontsize{26}{36}\selectfont}
\titlespacing{\chapter}{0 pt}{30 pt}{50 pt}[0 pt]
\titleformat{\section}{\Large\bfseries}{\thesection}{0 pt}{\hspace{30 pt}}
\titleformat{\subsection}{\large\bfseries}{\thesubsection}{0 pt}{\hspace{30 pt}}
\pagestyle{fancy}
\fancyhead[LO,LE]{\footnotesize\textit{\leftmark}}
\fancyhead[RO,RE]{\thepage}
\fancyfoot[CO,CE]{}
%Termina configuracion de capitulo

\chapter{State of the Art} %Cambia al nombre de tu capitulo
\setstretch{1.5} %Regresa el interlineado a 1.5

\normalsize

\section{Related Work}
\noindent


%introduction-> MPI

With the proliferation of high performance workstations and the emergence of
high speed networks in early 1990's, the rise of interest in parallel and
distributed computing was natural. The first clusters of computers start to
require a communication protocol to share the workload among them. In
\cite{Salim} one of the first solutions for that was presented: A message
passing interface (MPI) for parallel and distributed computing (PDC) .The main
objective of the PDC environment was to provide parallel and distributed
applications with message passing primitives as well as the host- network
interface required to achieve efficient environment over the emerging Gigabit
networks.

%MPICH
\subsection{Parallel and distributed computing for embedded}

After the publication of \cite{Salim} the first practical implementation of the
MPI protocol came: MPICH. MPICH is a high performance and widely portable
implementation of the Message Passing Interface (MPI) standard.\cite{Gropp}.
MPI was designed for high performance on both massively parallel machines and
on workstation clusters. Its fully open source version is also used nowadays:
Open MPI. The Open MPI project is an open source Message Passing Interface
(MPI) implementation that is developed and maintained by a consortium of
academic, research, and industry partners.

%First MPICH for Embedded

These MPI implementations are designed for cluster of high performance
computers. Is expected to have a hight amount of resources in terms of memory,
power consumption and a full operating system (able to handle processes and
threads with an schedule). Despite the quick adoption of these MPI
implementation in the high performance computing community, there was a group
of people interested in the same approach of parallel and distributed computing
applied in embedded systems: One of these first approaches was eMPICH.
\cite{McMahon}. In this early MPI implementation for embedded systems  two
design paradigms for embedding MPI are described: top-down and bottom-up.

\begin{itemize}
\item For the top-down approach, they concentrated on removing and simplifying
features of the full MPI implementations, and reorganizing the code in order to
decrease the amount of unnecessary code linked in to an application. Examples
of changes are: remove the support for data heterogeneity, removing the error
checking and reduce the multiple send modes. All these changes reduce the
complexity of MPI to send, receive and handle a package.

\item For the bottom-up approach, a full re write of the MPI protocol was
implemented. For the bottom-up approach, they began with a very small, six
function version of eMPICH, and then created a sequence of eMPICH libraries
of increasing size by adding various functionality to this "base" implementation
\end{itemize}

At the end \cite{McMahon} concludes than apart from  reducing the executable
code sizes of the MPI layer, there are a number of other issues that need
to be addressed within the scope of embedded MPI. Many of them are related with
the lack of proper operating system for embedded platforms. The operating
system is the one responsible to manage techniques for inter-processor
communication.  In order to create a parallel and distributed computing system
among embedded platforms is necessary to solve these problems first.

% Thread base solutions

\subsubsection{Thread base solutions}

The problem was that the mechanisms to achieve the implementation of MPI for
embedded platforms require extensive system resources (volatile and none
volatile memory as well as the use of a microprocessor instead of a low voltage
microcontroler) with comprehensive operating systems support. All these
elements were not available in an embedded platform at those years (end of
1990's). Is because of this that another approach came; thread base solutions.
One of the projects that follow this approach was AzequiaMPI \cite{Gallego}.

AzequiaMPI is an MPI implementation that uses threads instead of processes
making MPI applications more lightweight. The AzequiMPI protocol runs on the
entire Texas Instruments TMS320C6000 family of digital signal processors
(DSPs). \footnote{The DSP is a specialized microprocessor
with its architecture optimized for digital signal processing tasks. Thes DSPs
are broadly used in high performance real time signal processing
applications.}. Threads are used for small tasks, whereas processes are used
for more 'heavyweight' tasks. Another
difference between a thread and a process is that threads within the same
process share the same address space, whereas different processes do not

According to \cite{Gallego} running an MPI node as a process imposes some
disadvantages because firstly, process context switch and synchronization are
expensive, and secondly, message passing between two MPI nodes in the same
machine must go through a system buffer, affecting  the communication
efficiency. 

The idea that \cite{Gallego} proposed was radical, the fact of use threads
instead of processes, removes the need of a operating system. In the original
idea presented by \cite{Salim} the MPI node  is a process, this was followed by
all the mainstream implementations \footnote{MPI as MPICH and OpenMPI}. In
contrast, in \cite{Gallego} the MPI node is a thread in a thread-based
implementation.

Thread-based MPI is not a new concept. Paper \cite{Tang} proposes TMPI (for
threaded MPI), a partial implementation based on optimised run-time support
techniques by using threads in high end multiprocessors. 

Paper \cite{Demaine} discusses TOMPI (for thread only MPI), following an
approach similar to AzequiaMPI. TOMPI however is just an early proof of concept
prototype that implements just a reduced set of MPI primitives. 

Some tests (Figure 3 in \cite{Gallego}) compare the performance of AzequiaMPI,
TOMPI and MPICH in a single Linux PC. The results shows that the thread-based
implementations double the performance of MPICH, on other hand, the performance
of AzequiaMPI is quite similar to the TOMPI project. 

Is because of this results,  that in the end \cite{Gallego} conclude that a better
threads support would contribute to simplify the AzequiaMPI solution,
eliminating the need of an operating system, improving the performance and
reducing its memory footprint. 


%Process base approach
\subsubsection{Process base solution}

In paper \cite{Ziavrasa} a complete opposite approach is followed, implementing
a MPI "kernel" in hardware. This paper takes advantage of the effectiveness and
efficiency of one-sided Remote Memory Access (RMA)  \footnote{The remote direct
memory access (RDMA) is a direct memory access from the memory of one system
into the memory of another one without involving either one's operating system
during the process} communications, and presents the design and evaluation of a
coprocessor that implements a set of MPI primitives for RMA. These primitives
can be used to implement any other MPI function

Other papers \cite{Saldana-Chow} demonstrate that MPI is the right programming
model in order to build a hybrid cluster of embedded platforms  hiding hardware
complexities from the programmer and promoting code portability. In
\cite{Saldana-Chow} and \cite{Williams} is presented, a minimal subset of MPI
that does not requires an operating system. TMD-MPI however just supports a single thread of control in each  processor. 

%Why Process base wins

Despite the fact of the MPI implementation (either process-base or
thread-base), one thing is sure, in all of them is proved that when multiple
embedded processors are available, the workload can be distributed such that
each processor runs at a very low-power level; meanwhile the performance is
compensated by parallelism.  \cite{Weglarz} proposed partitioning the
computation onto a multi-processor architecture that consumes significantly
less power than a single processor.  

All these light and limited versions (either process-base or
thread base ) of MPI are relevant proofs of the raising role of MPI in embedded
world. However they have an issue: portability. All the MPI programs that run
in a traditional Linux cluster should also be able to run in a cluster of
embedded platforms  by just recompilation.  Even for \cite{Gallego} the support
of an operating system plays here seems to be essential if the portability is a
key part to consider.

\subsubsection{Lightweight Message Passing Interface}

One of the most powerful approaches for MPI implementations for embedded
systems is the Lightweight Message Passing Interface(LMPI)\cite{Abgaria}. The
idea of LMPI is separation of its server part (LMPI server) and the very thin
client part (LMPI client). Both parts can reside on different hardware or on
the same hardware. Multiple clients can be associated with a server. LMPI
servers support full capability of MPI and can be implemented using
pre-existing MPI implementation. 

Although LMPI is dedicated to embedded systems, to demonstrate the benefits of
LMPI and show some initial results, \cite{Abgaria} built an LMPI server using
MPICH on a non-embedded system. LMPI client consumes far less computation and
communication bandwidth than typical implementations of MPI, such as MPICH.  As
a result, LMPI client is suitable for embedded systems with limited computation
power and memory. They demonstrated the low overhead of LMPI clients on Linux
workstations, which is as low as 10\% of MPICH for two benchmark applications.
LMPI clients are highly portable because they don't rely on the operating
system support. All they require from the embedded system is networking support
to the LMPI server.

\subsection{Current clusters of embedded systems}

Thanks to all these works now is possible to see real distributed embedded
applications. In \cite{Liu} the author implemented an image Processing algorithm
with MPI on a low voltage computer. As a result of these article the author
confirmed that the high-level distributed programming tools are readily
available to facilitate the design and exploration of distributed embedded
applications. However emphasize that the communication mechanism of MPI needs
further study (LMPI was not implemented yet at that point). 

As we can see there is a need to make new experiments with the latest
technology available. Nowadays we have ultra-low-voltage microprocessors
platforms, customized operating systems for these platforms and  innovative
technologies for achieving MPI implementations. All these variables Will be
considered into the development of this current work.

\subsection{Latest Technology in Embedded Processors}



\subsection{Latest Technology in Embedded Operating Systems}

\newpage

\clearpage
